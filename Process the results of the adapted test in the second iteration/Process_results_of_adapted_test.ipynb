{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t8r2hl5nGBFv"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import google.colab.files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "TEST_MAX_SCORE = 15\n",
        "SHOW_LOGS = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UBxwbGnFj1M"
      },
      "source": [
        "# Upload file to process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NrpKHthE20u"
      },
      "outputs": [],
      "source": [
        "uploaded = google.colab.files.upload()\n",
        "file_name = list(uploaded.keys())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NkQWXR6GXvI"
      },
      "source": [
        "# Difficulty index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-jkrsNIsGWpH"
      },
      "outputs": [],
      "source": [
        "# Input: xlsx file with columns \"id\" (any type) and fifteen items which are 0 or 1\n",
        "results_df = pd.read_excel(\n",
        "    io=file_name,\n",
        "    header=0,\n",
        "    names=[\n",
        "        \"id\",\n",
        "        \"item1\",\n",
        "        \"item2\",\n",
        "        \"item3\",\n",
        "        \"item4\",\n",
        "        \"item5\",\n",
        "        \"item6\",\n",
        "        \"item7\",\n",
        "        \"item8\",\n",
        "        \"item9\",\n",
        "        \"item10\",\n",
        "        \"item11\",\n",
        "        \"item12\",\n",
        "        \"item13\",\n",
        "        \"item14\",\n",
        "        \"item15\",\n",
        "    ],\n",
        "    usecols=[2, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
        ")\n",
        "results_df = results_df.dropna(subset=[\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GjPXSub8UbX1"
      },
      "outputs": [],
      "source": [
        "# item_columns = [\"item1\", \"item2\", ...]\n",
        "item_columns = list(\n",
        "  filter(\n",
        "      lambda column_name: \"item\" in column_name, list(results_df.columns)\n",
        "  )\n",
        ")\n",
        "total_answer_per_item = {}\n",
        "correct_answer_per_item = {}\n",
        "for item in item_columns:\n",
        "  wrong_answer_in_item = results_df.loc[results_df[item] == 0, item].count()\n",
        "\n",
        "  correct_answer_in_item = results_df.loc[results_df[item] == 1, item].count()\n",
        "  correct_answer_per_item[item] = correct_answer_in_item\n",
        "\n",
        "  total_answer_per_item[item] = wrong_answer_in_item + correct_answer_in_item\n",
        "\n",
        "if SHOW_LOGS:\n",
        "  print(\"A, all items\", correct_answer_per_item)\n",
        "  print(\"N, all items\", total_answer_per_item)\n",
        "\n",
        "p = {}\n",
        "for item in item_columns:\n",
        "  A = correct_answer_per_item[item]\n",
        "  N = total_answer_per_item[item]\n",
        "  \n",
        "  p_i = [round(A / N, 2)]\n",
        "  p[item] = p_i\n",
        "\n",
        "  if SHOW_LOGS:\n",
        "    print(f\"{item} -> \\t\", \"A_i:\", A, \"\\tN_i:\", N, \"\\tp_i:\", p_i[0])\n",
        "\n",
        "difficulty_df = pd.DataFrame.from_dict(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZJP0iWzkbFR"
      },
      "source": [
        "# Discrimination index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AwPC4vc7vxyH"
      },
      "outputs": [],
      "source": [
        "# Input: xlsx file with columns \"id\" (any type), \"total_points\", and fifteen items which are 0 or 1\n",
        "results_df = pd.read_excel(\n",
        "    io=file_name,\n",
        "    header=0,\n",
        "    names=[\n",
        "        \"id\",\n",
        "        \"total_points\",\n",
        "        \"item1\",\n",
        "        \"item2\",\n",
        "        \"item3\",\n",
        "        \"item4\",\n",
        "        \"item5\",\n",
        "        \"item6\",\n",
        "        \"item7\",\n",
        "        \"item8\",\n",
        "        \"item9\",\n",
        "        \"item10\",\n",
        "        \"item11\",\n",
        "        \"item12\",\n",
        "        \"item13\",\n",
        "        \"item14\",\n",
        "        \"item15\",\n",
        "    ],\n",
        "    usecols=[2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
        ")\n",
        "results_df_original = copy.deepcopy(results_df)\n",
        "results_df = results_df.dropna(subset=[\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eBDxYS6mgEy6"
      },
      "outputs": [],
      "source": [
        "sorted_results_df = results_df.sort_values(by=['total_points'], ascending=False)\n",
        "twenty_seven_percentage = round(results_df.shape[0] * 0.27)\n",
        "twenty_seven_percentage_upper_group_df = sorted_results_df[:twenty_seven_percentage]\n",
        "twenty_seven_percentage_lower_group_df = sorted_results_df[-twenty_seven_percentage:]\n",
        "\n",
        "item_columns = list(\n",
        "  filter(\n",
        "      lambda column_name: \"item\" in column_name, list(results_df.columns)\n",
        "  )\n",
        ")\n",
        "\n",
        "n_people_largest_group_per_item = {}\n",
        "correct_answer_upper_group_per_item = {}\n",
        "correct_answer_lower_group_per_item = {}\n",
        "for item in item_columns:\n",
        "  correct_answer_upper_group_in_item = twenty_seven_percentage_upper_group_df.loc[twenty_seven_percentage_upper_group_df[item] == 1, item].count()\n",
        "  correct_answer_lower_group_in_item = twenty_seven_percentage_lower_group_df.loc[twenty_seven_percentage_lower_group_df[item] == 1, item].count()\n",
        "  \n",
        "  correct_answer_upper_group_per_item[item] = correct_answer_upper_group_in_item\n",
        "  correct_answer_lower_group_per_item[item] = correct_answer_lower_group_in_item\n",
        "\n",
        "  n_people_largest_group_per_item[item] = (\n",
        "      correct_answer_upper_group_in_item\n",
        "      if correct_answer_upper_group_in_item >= correct_answer_lower_group_in_item\n",
        "      else correct_answer_lower_group_in_item\n",
        "  )\n",
        "\n",
        "\n",
        "if SHOW_LOGS:\n",
        "  print(\"GA, all items\", correct_answer_upper_group_per_item)\n",
        "  print(\"GB, all items\", correct_answer_lower_group_per_item)\n",
        "  print(\"N, all items\", n_people_largest_group_per_item)\n",
        "\n",
        "D = {}\n",
        "for item in item_columns:\n",
        "  GA = correct_answer_upper_group_per_item[item]\n",
        "  GB = correct_answer_lower_group_per_item[item]\n",
        "  N = n_people_largest_group_per_item[item]\n",
        "\n",
        "  # Cambiar para que el N sea siempre el mismo: twenty_seven_percentage\n",
        "  D_i = [round((GA - GB) / twenty_seven_percentage, 2)]\n",
        "  D[item] = D_i\n",
        "\n",
        "  if SHOW_LOGS:\n",
        "    print(f\"{item} -> \\t\", \"GA_i:\", GA, \"\\t\\tGB_i:\", GB, \"\\t\\tN_i:\", N, \"\\t\\tD_i:\", D_i[0])\n",
        "\n",
        "if SHOW_LOGS:\n",
        "  print(\"D, all items:\", D)\n",
        "\n",
        "discrimination_df = pd.DataFrame.from_dict(D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QAU0_JpHAV"
      },
      "source": [
        "# Cronbach $\\alpha$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN2BE6Q5z_Ug"
      },
      "outputs": [],
      "source": [
        "# items is a dataframe with items in columns\n",
        "def cronbach_alpha(items):\n",
        "\n",
        "    # Based on Cronbach (1951), \"Coefficient alpha and the internal structure of tests\"\n",
        "\n",
        "    items_count = items.shape[1]\n",
        "    item_score_variance_sum = float(items.var(axis=0, ddof=1).sum()) # sum of variance of item i\n",
        "    test_scores_variance = float(items.sum(axis=1).var(ddof=1)) # variance of the sum of total points of students\n",
        "    K = items_count\n",
        "\n",
        "    alpha = (K / (K - 1)) * (1 - (item_score_variance_sum / test_scores_variance))\n",
        "    \n",
        "    return alpha\n",
        "\n",
        "# Input: xlsx file with columns \"id\" (any type), and fifteen items which are 0 or 1\n",
        "results_df = pd.read_excel(\n",
        "    io=file_name,\n",
        "    header=0,\n",
        "    names=[\n",
        "        \"id\",\n",
        "        \"item1\",\n",
        "        \"item2\",\n",
        "        \"item3\",\n",
        "        \"item4\",\n",
        "        \"item5\",\n",
        "        \"item6\",\n",
        "        \"item7\",\n",
        "        \"item8\",\n",
        "        \"item9\",\n",
        "        \"item10\",\n",
        "        \"item11\",\n",
        "        \"item12\",\n",
        "        \"item13\",\n",
        "        \"item14\",\n",
        "        \"item15\",\n",
        "    ],\n",
        "    usecols=[2, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
        ")\n",
        "results_df_original = copy.deepcopy(results_df)\n",
        "# drop rows with null ID\n",
        "results_df = results_df.dropna(subset=[\"id\"])\n",
        "results_df.drop(\n",
        "    [\"id\"], axis=1, inplace=True,\n",
        ")\n",
        "# Update \"-\"\" to None\n",
        "results_df = results_df.replace(\"-\", None)\n",
        "\n",
        "cronbach_alpha_result = cronbach_alpha(results_df)\n",
        "\n",
        "if SHOW_LOGS:\n",
        "  print(\"cronbach_alpha: \", cronbach_alpha_result)\n",
        "\n",
        "cronbach_alpha_df = pd.DataFrame.from_dict({\"Cronbach alpha\": [cronbach_alpha_result]})\n",
        "cronbach_alpha_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8uDhWHh6qzG"
      },
      "source": [
        "# Measures of central tendency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iQvbFwQk6wq2"
      },
      "outputs": [],
      "source": [
        "# Input: xlsx file with columns \"id\" (any type), \"total_points\", and fifteen items which are 0 or 1\n",
        "results_df = pd.read_excel(\n",
        "    io=file_name,\n",
        "    header=0,\n",
        "    names=[\n",
        "        \"id\",\n",
        "        \"total_points\",\n",
        "        \"item1\",\n",
        "        \"item2\",\n",
        "        \"item3\",\n",
        "        \"item4\",\n",
        "        \"item5\",\n",
        "        \"item6\",\n",
        "        \"item7\",\n",
        "        \"item8\",\n",
        "        \"item9\",\n",
        "        \"item10\",\n",
        "        \"item11\",\n",
        "        \"item12\",\n",
        "        \"item13\",\n",
        "        \"item14\",\n",
        "        \"item15\",\n",
        "    ],\n",
        "    usecols=[2, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22],\n",
        ")\n",
        "results_df_original = copy.deepcopy(results_df)\n",
        "results_df = results_df.dropna(subset=[\"id\"])\n",
        "\n",
        "results_df = results_df.replace(\"-\", 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWHo3zyfdkhC"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 600)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4J9SLC9-9Rb"
      },
      "source": [
        "## Statistics by item for a specific site and race (file to be processed has only this data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl-jkx8BBp-d"
      },
      "outputs": [],
      "source": [
        "# Obtain central tendency values for score data\n",
        "\n",
        "central_tendency_df = results_df.drop(results_df.index)\n",
        "central_tendency_df.loc['mean'] = results_df.mean(axis = 0, numeric_only=True)\n",
        "central_tendency_df.loc['std'] = results_df.std(axis = 0, numeric_only=True)\n",
        "central_tendency_df.loc['median'] = results_df.median(axis = 0, numeric_only=True)\n",
        "central_tendency_df.loc['min'] = results_df.min(axis = 0, numeric_only=True)\n",
        "central_tendency_df.loc['max'] = results_df.max(axis = 0, numeric_only=True)\n",
        "central_tendency_df.drop(\n",
        "    [\"id\"], axis=1, inplace=True,\n",
        ")\n",
        "\n",
        "# Round values\n",
        "for column in results_df.columns:\n",
        "  # Avoid processing column which are not an item score\n",
        "  try:\n",
        "    central_tendency_df[column] = central_tendency_df[column].map(\n",
        "        lambda score: round(score, 2)\n",
        "    )\n",
        "  except KeyError:\n",
        "    pass\n",
        "\n",
        "central_tendency_df = central_tendency_df.rename(\n",
        "    columns={\n",
        "        \"total_points\": \"Mark/15\",\n",
        "        \"item1\": \"Item 1\",\n",
        "        \"item2\": \"Item 2\",\n",
        "        \"item3\": \"Item 3\",\n",
        "        \"item4\": \"Item 4\",\n",
        "        \"item5\": \"Item 5\",\n",
        "        \"item6\": \"Item 6\",\n",
        "        \"item7\": \"Item 7\",\n",
        "        \"item8\": \"Item 8\",\n",
        "        \"item9\": \"Item 9\",\n",
        "        \"item10\": \"Item 10\",\n",
        "        \"item11\": \"Item 11\",\n",
        "        \"item12\": \"Item 12\",\n",
        "        \"item13\": \"Item 13\",\n",
        "        \"item14\": \"Item 14\",\n",
        "        \"item15\": \"Item 15\",\n",
        "    }\n",
        ")\n",
        "\n",
        "central_tendency_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLZyB1m5vCac"
      },
      "source": [
        "## Statistics by dimension for a specific site and race (file to be processed has only the data for a site and race, and includes the scores in each item)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKoSzy_cJlip"
      },
      "source": [
        "### Mapping dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8WelDRVJfszO"
      },
      "outputs": [],
      "source": [
        "# Manual mapping of dimensions\n",
        "dimensions_item_map = {\n",
        "    \"Abstraction [%]\": [\n",
        "        \"item2\",\n",
        "        \"item3\",\n",
        "        \"item4\",\n",
        "        \"item5\",\n",
        "        \"item6\",\n",
        "        \"item12\",\n",
        "        \"item13\",\n",
        "        \"item14\",\n",
        "        \"item15\"\n",
        "    ],\n",
        "    \"Pattern rec. [%]\": [\n",
        "        \"item3\",\n",
        "        \"item4\",\n",
        "        \"item5\",\n",
        "        \"item9\",\n",
        "        \"item10\",\n",
        "        \"item11\"\n",
        "    ],\n",
        "    \"Algorithms [%]\": [\n",
        "        \"item1\",\n",
        "        \"item6\",\n",
        "        \"item7\",\n",
        "        \"item8\",\n",
        "        \"item12\",\n",
        "        \"item13\",\n",
        "        \"item14\",\n",
        "        \"item15\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "dimensions_item_map_names = [\"Abstraction\", \"Pattern recognition\",  \"Algorithms\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd6STAxyJ5zZ"
      },
      "source": [
        "### Obtain score by dimension, in percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuZjGdW9H5bb"
      },
      "outputs": [],
      "source": [
        "result_df_by_dim = copy.deepcopy(results_df)\n",
        "global items_columns\n",
        "\n",
        "# Funtion to group scores by dimension\n",
        "def parse_row_item_points_to_dim(row):\n",
        "\n",
        "  if SHOW_LOGS: print(\"id:\", row[\"id\"])\n",
        "\n",
        "  for column in items_columns:\n",
        "\n",
        "    if SHOW_LOGS: print(\"Column:\", column)\n",
        "    \n",
        "    for dim, items_in_dim_list in dimensions_item_map.items():\n",
        "\n",
        "      if SHOW_LOGS: print(\"Dim:\", dim)\n",
        "\n",
        "      if column in items_in_dim_list:\n",
        "        row[dim] += row[column]\n",
        "\n",
        "    if SHOW_LOGS: print(\"\\n\")\n",
        "\n",
        "  return row\n",
        "\n",
        "items_columns = []\n",
        "for column in result_df_by_dim.columns:\n",
        "  # Avoid processing column which are not an item score\n",
        "  if not re.search(\"item(?:\\d+)\", column):\n",
        "    continue\n",
        "  items_columns.append(column)\n",
        "\n",
        "for dim in dimensions_item_map.keys():\n",
        "  result_df_by_dim[dim] = 0\n",
        "\n",
        "result_df_by_dim = result_df_by_dim.apply(parse_row_item_points_to_dim, axis=1)\n",
        "\n",
        "# Delete normal score columns (columns with score 0 or 1)\n",
        "result_df_by_dim.drop(\n",
        "    items_columns, axis=1, inplace=True,\n",
        ")\n",
        "\n",
        "# Standardize data by dimension, making columns relative to total quantity of\n",
        "# element in each dimension and give final value in percentage\n",
        "for dim in dimensions_item_map.keys():\n",
        "  result_df_by_dim[dim] = result_df_by_dim[dim].map(\n",
        "      lambda dim_score: (dim_score / len(dimensions_item_map[dim])) * 100\n",
        "  )\n",
        "result_df_by_dim[\"total_points\"] = result_df_by_dim[\"total_points\"].map(\n",
        "      lambda total_score: (total_score / TEST_MAX_SCORE) * 100\n",
        "  )\n",
        "\n",
        "# Show processed data with values in percentage\n",
        "result_df_by_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFRPaozqK2ia"
      },
      "outputs": [],
      "source": [
        "# Obtain central tendency values for score by dimension data\n",
        "central_tendency_df_by_dim = result_df_by_dim.drop(results_df.index)\n",
        "central_tendency_df_by_dim.loc['mean'] = result_df_by_dim.mean(axis = 0, numeric_only=True)\n",
        "central_tendency_df_by_dim.loc['std'] = result_df_by_dim.std(axis = 0, numeric_only=True)\n",
        "central_tendency_df_by_dim.loc['median'] = result_df_by_dim.median(axis = 0, numeric_only=True)\n",
        "central_tendency_df_by_dim.loc['min'] = result_df_by_dim.min(axis = 0, numeric_only=True)\n",
        "central_tendency_df_by_dim.loc['max'] = result_df_by_dim.max(axis = 0, numeric_only=True)\n",
        "central_tendency_df_by_dim.drop(\n",
        "    [\"id\"], axis=1, inplace=True,\n",
        ")\n",
        "\n",
        "# Round values\n",
        "for dim in dimensions_item_map.keys():\n",
        "  central_tendency_df_by_dim[dim] = central_tendency_df_by_dim[dim].map(\n",
        "      lambda dim_score: round(dim_score, 2)\n",
        "  )\n",
        "central_tendency_df_by_dim[\"total_points\"] = central_tendency_df_by_dim[\"total_points\"].map(\n",
        "    lambda score: round(score, 2)\n",
        ")\n",
        "central_tendency_df_by_dim = central_tendency_df_by_dim.rename(\n",
        "    columns={'total_points': 'Mark/15 [%]'}\n",
        ")\n",
        "\n",
        "central_tendency_df_by_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqLC5s_y_Aul"
      },
      "source": [
        "### Plotting data dispersion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpXWz9sC6yR_"
      },
      "outputs": [],
      "source": [
        "# BLACK GRAPH\n",
        "np.random.seed(2020)\n",
        "result_df_by_dim.rename(\n",
        "    columns={\n",
        "        \"Abstraction [%]\": \"Abstraction\",\n",
        "        \"Pattern rec. [%]\": \"Pattern recognition\",\n",
        "        \"Algorithms [%]\": \"Algorithms\",\n",
        "    },\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "boxprops = dict(linestyle='-', linewidth=2, color='#277fab')\n",
        "medianprops = dict(linestyle='--', linewidth=3, color='g')\n",
        "capprops = dict(linestyle='-', linewidth=2, color='k')\n",
        "\n",
        "boxplot = result_df_by_dim.boxplot(\n",
        "    grid=False, \n",
        "    column=dimensions_item_map_names,\n",
        "    figsize=(12, 10),\n",
        "    fontsize=13,\n",
        "    medianprops=medianprops,\n",
        "    boxprops=boxprops,\n",
        "    whiskerprops=boxprops,\n",
        "    capprops=capprops\n",
        ")\n",
        "boxplot.set_xlabel('Dimension', fontsize=14, fontdict=dict(weight='bold'))\n",
        "boxplot.set_ylabel('Achievement [%]', fontsize=14, fontdict=dict(weight='bold'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QSwU0hZ0Su0"
      },
      "outputs": [],
      "source": [
        "# Grayscale image\n",
        "np.random.seed(2020)\n",
        "result_df_by_dim.rename(\n",
        "    columns={\n",
        "        \"Abstraction [%]\": \"Abstraction\",\n",
        "        \"Pattern rec. [%]\": \"Pattern rec.\",\n",
        "        \"Algorithms [%]\": \"Algorithms\",\n",
        "    },\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "boxprops = dict(linestyle='-', linewidth=2, color='k')\n",
        "medianprops = dict(linestyle='--', linewidth=3, color='gray')\n",
        "capprops = dict(linestyle='-', linewidth=2, color='gray')\n",
        "color = dict(boxes='black', whiskers='black', medians='black', caps='#808080')\n",
        "\n",
        "boxplot_grayscale = result_df_by_dim.boxplot(\n",
        "    grid=False, \n",
        "    column=dimensions_item_map_names,\n",
        "    figsize=(12, 10),\n",
        "    fontsize=13,\n",
        "    medianprops=medianprops,\n",
        "    boxprops=boxprops,\n",
        "    whiskerprops=boxprops,\n",
        "    capprops=capprops,\n",
        "    color=color\n",
        ")\n",
        "boxplot_grayscale.set_xlabel('Dimension', fontsize=14)\n",
        "boxplot_grayscale.set_ylabel('Achievement [%]', fontsize=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAz4Y0y0Iwq4"
      },
      "source": [
        "### Download boxplot as image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EguPhVP5EmRZ"
      },
      "outputs": [],
      "source": [
        "boxplot.figure.savefig(\n",
        "    \"Boxplot percentage of achievement by dimension.jpg\",\n",
        "    format='jpeg',\n",
        "    dpi=200\n",
        ")\n",
        "\n",
        "boxplot_grayscale.figure.savefig(\n",
        "    \"Boxplot percentage of achievement by dimension grayscale.jpg\",\n",
        "    format='jpeg',\n",
        "    dpi=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ih1D0srGGBm"
      },
      "source": [
        "# Download processed file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiPRHXRw9sXj"
      },
      "outputs": [],
      "source": [
        "preprocessed_results_df = pd.read_excel(\n",
        "    io=file_name,\n",
        "    header=0,\n",
        ")\n",
        "\n",
        "with pd.ExcelWriter(f\"Processed {file_name}\") as writer:  \n",
        "    preprocessed_results_df.to_excel(writer, sheet_name=\"Pre-processed results\", index=False)\n",
        "    difficulty_df.to_excel(writer, sheet_name=\"Difficulty index\", index=False)\n",
        "    discrimination_df.to_excel(writer, sheet_name=\"Discrimination index\", index=False)\n",
        "    central_tendency_df.to_excel(writer, sheet_name=\"Measures by item\", index=True)\n",
        "    central_tendency_df_by_dim.to_excel(writer, sheet_name=\"Measures by dimension\", index=True)\n",
        "    cronbach_alpha_df.to_excel(writer, sheet_name=\"Cronbach alpha\", index=False)\n",
        "\n",
        "google.colab.files.download(f\"Processed {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iuF06KpIlN1"
      },
      "outputs": [],
      "source": [
        "result_df_by_dim.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7Ga-6Mp5Oe7U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "VF Procesamiento resultados Test Diagnóstico.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}